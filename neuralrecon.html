<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Alexiy's Blog Page" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <link href='https://fonts.googleapis.com/css?family=Roboto:400,400italic,700italic,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Condensed:300,300italic,700,700italic' rel='stylesheet' type='text/css'>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <title>Paper Analysis: NeuralRecon</title>
  </head>

  <body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
        </header>
    </div>

    <!-- CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <!-- TITLE -->
        <img alt="a Wild Me appears!" class="myimage" src="./images/me.jpeg"/>
        <p id="name">Alexiy Zhandarov</p>
        <p id="undername">Informatics B.Sc Student</p>

        <a id="home_link" href="index.html"><u>Home</u></a>
        <hr id=startmargin>

        <!-- ACTUAL START -->
        <h1 style="text-align:center">Paper Analysis on Real-Time 3D Scene Reconstruction from Monocular Video</h1>
        <p class="time"> May 20, 2022 </p>

        <!-- QUICK INTRO -->
        <p> In this blog post, I will review and analyze the paper NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video published in CVPR 2021 [<a href="#References">1</a>]
        After a brief introduction and going over the relevant terms, I will attempt to explain the algorithm and framework as simply as possible, observe an example, and finally dissect the results and see what it all means for the future of computer vision. </p>

        <video width="100%" height="250" autoplay muted>
          <source src="./images/web-scene2.m4v" type="video/mp4">
              Your browser does not support the video format.
              Try Using Chrome on PC.
          </video>
          <p class="explain"> Showcase of the proposed framework.[<a href="#References">2</a>] </p>


        <!-- INTRODUCTION -->
        <h3>Introduction</h3>
        <p>
          Scene reconstruction is one of the central tasks in 3D computer vision with currently numerous applications. 3D reconstruction has seen a wide range of usage within the medical industry, computer animation and graphics, virual reality(VR) and augmented reality(AR).
        </p>
        <p>
          Focusing in on the latter for example, to enable realistic and immersive interactions between the desired effects and the surrounding physical environment, the 3D reconstruction has many standards to fulfil; it needs to be accurate, coherent and performed without any noticeable delay.
        </p>
        <p>
          While camera motion can be tracked accurately with state-of-the-art visual-inertial SLAM systems [<a href="#References">3</a>], real-time image-based dense reconstruction remains to be a challenging problem due to <b>low reconstruction quality and high computation demands.</b> Slow and low quality algorithms are naturaly undesirable and can be unreliable when requiring precise results. The proposed framework 'NeuralRecon' attempts to address these issues. Moreover the method proposes a way avoiding the common path of depth-based methods, which uses TSDF fusion and per frame analyzation, and rather predicts it in a local window.
        </p>
        <img alt="result1" class="results" src="./images/results1.jpg"/>
        <p class="explain">
          Figure 1. Comparison between depth-based methods and the proposed 3D reconstruction method.
        </p>

        <!-- Visual Odometry and SLAM -->
        <h3>Visual Odometry and SLAM</h3>
        <p>
          Visual odometry(VO), one of the founding stones in computer vision, is the process of determining the position and orientation of an actor by analyzing the information provided by its sensors/cameras. Its uses vary immensely from the average drone to the Mars Rovers Exploration[<a href="#References">10</a>].
        </p>
        <p>
          There are many types of VO, we will be focusing on Monocular VO(opposed to Stereo VO) and visual-inertial odometry. Where Monocular VO refers to getting all the input from a single source (a single camera) and visual-inertial odometry to a system empolying an inertial measurement unit (IMU).
        </p>
        <p>
          Simultaneous localization and mapping (SLAM) as the name suggests involves a computational process of constructing a map of an unknown environment while simultaneously keeping track of the agent's location on said map.
        </p>

        <!-- 3D reconstruction -->
        <h3>3D reconstruction</h3>
        <p>
          Reconstruction refers to the process of capturing the shape and appearance of objects. This allows to determine the object's profile and creating a 3D grid.
          While many methods are possible to achieve this, the discussion primarily focuses on the Monocular aspect, also called Monocular cues methods. These refer to the usage of one or more images from one viewpoint to create a 3D construction. Exploiting 2D characteristics and defining features(such as shadows and textures) to define the object of a higher dimension.
        </p>

        <!-- TSDF -->
        <h3>TSDF Volume</h3>
        <p>
          A Truncated Signed Distance Field (TSDF) is a voxel array representing objects within a volume of space in which each voxel is labeled with the distance to the nearest surface. Numerous observations of an object from different perspectives average out noise and errors due to specular highlights and interreflections, producing a smooth continuous surface.<b> The TSDF algorithm can be efficiently parallelized on a general-purpose graphics processor, which allows data from RGB-D cameras to be integrated into the volume in real time.</b> The volume can be converted to a triangular mesh using the <b>Marching Cubes algorithm</b> (explained in "Depth maps and computations") and then handed off to application-specific processes.
        </p>
        <p>
          [<a href="#References">11</a>]A <i>d</i>-dimensional grid of voxels, where the position of each voxel <b>x</b> is defined by its center.
        </p>
        <img alt="tsdf1" class="functions" src="./images/tsdf1.png"/>
        <p class="explain">
          Function 1
        </p>
        <p>
          <i>sdf</i>, which is the signed distance in between voxel center and nearest object surface in direction of current measurement.
          Objects in front of itself are positive whereas the area inside the object is defined as negative. <i> i </i> refers to the <i> i </i>-th observation.
        </p>
        <p>
          <i> pic(<b>x</b>)</i> denotes to the projection of the voxel center onto the depth image. Accordingly <i> depth </i> is the measured depth in between the camera and the nearest object surface point. <i> cam </i> refers to the distance in between the voxel and the camera along the optical axis.
        </p>
        <p>
          The truncated variant of <i>sdf</i> is denoted by function 2:
        </p>
        <img alt="tsdf2" class="functions" src="./images/tsdf2.png"/>
        <p class="explain">
          Function 2
        </p>
        <p> Each voxel has a weight <i>w</i> atteched to it to asses uncertainty of the corresponding <i>sdf</i>.
        </p>
        <p>
          Finally, as mentioned prior, multiple viewpoints can be fused together in one <i>TSDF</i> to improve accuracy or added missing surface. This is done by weighted summation.
        </p>
        <img alt="tsdf3" class="functions" src="./images/tsdf3.png"/>
        <p class="explain">
          Function 3 and 4 respectively
        </p>
        <!-- Depth maps and computations -->
        <h3>Depth maps and computations</h3>
        <p>
          Depth maps are a vital instrument in attempting to recreate a model from images. They are clearly useful in a wide range of fields, however tend to set back the process' speed by their high number of computations. Most image-based real-time pipelines adopt the depth map fusion approach[<a href="#References">4</a>].
        </p>
        <p>
          Single-view depth maps are first estimated from each key frame and filtered for consistency and temporal smoothness, and lastely fused into a Truncated Signed Distance Function Volume (TSDF-V). <br>
          The reconstructed mesh can be then extracted from the fusion with the marching Cubes Algoorithm [<a href="#References">5</a>]. An Algorithm, originally intended for usage in medical imaging, using a divide-and-conquer approach to generate inter-slice connectivity, defining a triangle topology it processes 3D data in-order and calculates verticies using linear interpolation. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data.
        </p>
        <p>
          This approach suffers as the maps are estimated individually on each key frame. Additionaly each map is estimated in full leading to substantial overlap in the results.
          Estimating the same surfaces multiple times causes unnecessary computations, and may cause difficulties delivering correct results in certain areas.(see Figure 1)
        </p>
        <p>
          The paper proposes a method to combat these problems fitting real-time reconstruction from a monocular video. By jointly reconstructing and fusing the 3D geometry directly in the TSDF-V and a given set of monocular images and camera poses from a SLAM system, NeuralRecon can incrementally unproject the image features to form sequentially a 3D feature volume by utilising convolutions to process the feature volume and eventually outputting a sparse TSDF-V which is then gradually refined.
        </p>
        <p>
          By directly reconstructing the implicit surface, the network is able to learn the local smoothness and global shape prior of natural 3D surfaces. This helps solve one of the issues introduced earlier of predicting separate depth maps.
        </p>

        <!-- GRU -->
        <h3>GRU</h3>
        <p>
          To make the jointly produced shape globally consistent with the previous reconstructions a learning based TSDF fusion module using the Gated Recurrent Unit(GRU) is proposed.
        </p>
        <p>
          While previous works commonly referred to SLTM for modeling, here a decision has been made to opt for a GRU based approach instead. GRU have been shown to be capabele of producing often comparable results with significantly faster computation times thanks to its simplicity. [<a href="#References">6,7</a>]
        </p>
        <img alt="GRU" class="results" src="./images/GRU.png">
        <p class="explain">
          Figure 2. LSTM and GRU graphical illustrations side by side [6].
        </p>
        <p>
          The GRU fusion makes the current-fragment reconstruction conditioned on the previously reconstructed global volume, hence creating a clear connection between the fragments correcting the problem, which appeared in other designs and yielding a joint reconstruction. This as a result, reconstructs a dense, accurate and globally coherent mesh. For exact calculation formulas see [6], however the main takeaway is, that similarly to the LSTM unit, the GRU has gating units that modulate the flow of information inside the unit, however, without having a separate memory cells. The GRU is forced to expose its full content and can control the flow of information when computing a new candidate.
        </p>
        <p>
          The experiments go on to report, that the new predicition procedure also removes the redundant computation in depth-based methods, which allows for the use of larger 3D CNNs while maintaining real-time performance.
        </p>

        <!-- CNN -->
        <h3>3D CNN</h3>
        <p>
          To validate the system ScanNet and 7-Scenes datasets have been used. Using
        </p>

        <!-- DATASETS -->
        <h3>Convolution</h3>
        <p>
          To validate the system ScanNet and 7-Scenes datasets have been used. Using
        </p>


        <!-- DATASETS -->
        <h3>Datasets</h3>
        <p>
          To validate the system ScanNet and 7-Scenes datasets have been used. Using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks. However in this case it's put into action to validate the results from the experiments I'll mention shortly, measure the correctness and also see how the framework performs when placed in comparison to current state-of-the-art methods.
        </p>
        <img alt="ScanNet" class="results" src="./images/dataset1.jpg"/>
        <h5>1. ScanNet</h5>
        <p>
          ScanNet is a RGB-D video dataset containing 2.5M views in over a thousand scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations [<a href="#References">8</a>].
        </p>
        <h5>2. 7-Scenes</h5>
        <p>
          The 7-Scenes dataset is a collection of tracked RGB-D camera frames. It can be used for evaluation of methods for uses such as dense tracking, mapping and more. [<a href="#References">9</a>]
        </p>



        <!-- Algorithm -->
        <h3>The algorithm</h3>
        ...

        <!-- Experiments -->
        <h3>The Experiments</h3>
        ...

        <!-- Experiments -->
        <h3>Results & Discussion</h3>
        ...

        <!-- References -->
        <div id="References">
        <h5><a href="#">References</a></h5>
        <ol>
            <li><a href="https://arxiv.org/pdf/2104.00681.pdf">Jiaming Sun, Yiming Xie1,Linghao Chen, Xiaowei Zhou, Hujun Bao, Zhejiang University, SenseTime Research - NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video</a></li>
            <li><a href="https://zju3dv.github.io/neuralrecon/">NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video Presentation Page</a></li>
            <li><a href="https://arxiv.org/pdf/2007.11898.pdf">Carlos Campos, Richard Elvira, Juan J. G´omez Rodr´ıguez, Jos´e M. M. Montiel, and Juan D. Tard´os. ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. ArXiv, 2020.</a></li>
            <li><a href="https://www.researchgate.net/publication/284139276_3D_Modeling_on_the_Go_Interactive_3D_Reconstruction_of_Large-Scale_Scenes_on_Mobile_Devices">Thomas Schops, Torsten Sattler, Christian Hane, and Marc
              Pollefeys. 3D Modeling on the Go: Interactive 3D Reconstruction
              of Large-Scale Scenes on Mobile Devices. In 3DV,
              2015.</a></li>
            <li><a href="https://dl.acm.org/doi/pdf/10.1145/37402.37422">William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. SIGGRAPH, 1987</a></li>
            <li><a href="https://arxiv.org/pdf/1412.3555.pdf">Junyoung Chung Caglar Gulcehre, Kyung Hyun Cho Universite de Montreal, Yoshua Bengio Universite de Montreal CIFAR Senior Fellow Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
            </a></li>
            <li><a href="https://arxiv.org/pdf/1409.1259.pdf">Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
            </a></li>
            <li><a href="https://arxiv.org/pdf/1702.04405">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner
            </a></li>
            <li>Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images. In CVPR, 2013</li>
            <li><a href="https://onlinelibrary.wiley.com/doi/10.1002/rob.20184">Maimone, M.; Cheng, Y.; Matthies, L. (2007). "Two years of Visual Odometry on the Mars Exploration Rovers"</a></li>
            <li><a href="https://www.researchgate.net/publication/268513443_Truncated_Signed_Distance_Function_Experiments_on_Voxel_Size">Werner, Diana & Al-Hamadi, Ayoub & Werner, Philipp. (2014). Truncated Signed Distance Function: Experiments on Voxel Size. 8815. 357-364. 10.1007/978-3-319-11755-3_40.</a></li>
          </ol>
        </div>
      </section>


    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
        <footer class="inner">
          <p class="copyright">Theme heavily inspired by <a href="https://github.com/jasoncostello">Jason Costello</a></p>
          <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
        </footer>
      </div>

  </body>
</html>
