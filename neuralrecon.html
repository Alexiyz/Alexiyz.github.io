<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Alexiy's Blog Page" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <link href='https://fonts.googleapis.com/css?family=Roboto:400,400italic,700italic,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Condensed:300,300italic,700,700italic' rel='stylesheet' type='text/css'>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <title>Paper Analysis: NeuralRecon</title>
  </head>

  <body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
        </header>
    </div>

    <!-- CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <!-- TITLE -->
        <img alt="a Wild Me appears!" class="myimage" src="./images/me.jpeg"/>
        <p id="name">Alexiy Zhandarov</p>
        <p id="undername">Informatics B.Sc Student</p>

        <a id="home_link" href="index.html"><u>Home</u></a>
        <a id="home_link" href="#References"><u>Bottom</u></a>
        <hr id=startmargin>

        <!-- ACTUAL START -->
        <h1 style="text-align:center">Paper Analysis on Real-Time 3D Scene Reconstruction from Monocular Video</h1>
        <p class="time"> May 20, 2022 </p>

        <!-- QUICK INTRO -->
        <p> In this blog post, I will review and analyze the paper NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video published in CVPR 2021 [<a href="#References">1</a>]
        After a brief introduction and going over the relevant terms, I will attempt to explain the algorithm and framework as simply as possible, observe an example, and finally dissect the results and see what it all means for the future of computer vision. </p>

        <video width="100%" height="250" autoplay muted>
          <source src="./images/web-scene2.m4v" type="video/mp4">
              Your browser does not support the video format.
              Try Using Chrome on PC.
          </video>
          <p class="explain"> Showcase of the proposed framework.[<a href="#References">2</a>] </p>


        <!-- INTRODUCTION -->
        <h2>Introduction</h2>
        <p>
          Scene reconstruction is one of the central tasks in 3D computer vision with currently numerous applications. 3D reconstruction has seen a wide range of usage within the medical industry, computer animation and graphics, virual reality(VR) and augmented reality(AR).
        </p>
        <p>
          Focusing in on the latter for example, to enable realistic and immersive interactions between the desired effects and the surrounding physical environment, the 3D reconstruction has many standards to fulfil; it needs to be accurate, coherent and performed without any noticeable delay.
        </p>
        <p>
          While camera motion can be tracked accurately with state-of-the-art visual-inertial SLAM systems [<a href="#References">3</a>], real-time image-based dense reconstruction remains to be a challenging problem due to <b>low reconstruction quality and high computation demands.</b> Slow and low quality algorithms are naturaly undesirable and can be unreliable when requiring precise results. The proposed framework 'NeuralRecon' attempts to address these issues. Moreover the method proposes a way avoiding the common path of depth-based methods, which uses TSDF fusion and per frame analyzation, and rather predicts it in a local window.
        </p>
        <img alt="result1" class="results" src="./images/results1.jpg"/>
        <p class="explain">
          Figure 1. Comparison between depth-based methods and the proposed 3D reconstruction method.
        </p>

        <!-- Main Contributions -->
        <h2>Main Contributions</h2>
        <p>
          NeuralRecon is a relatively new work being introduced at CVPR 2021, but is already showing some impressive results reconstructing 3D indoor scenes at real time and with high quality, being compratively equal or better than the current state-of-the-art frameworks. Here the aspect of speed comes as most impressive, as through a new prediction based procedure replacing the old depth maps used in similar works the time needed to generate coherent 3D models has been drastically reduced. (See Figure 1), Most importantly, with this new found speed, and quality of the constructions has not suffered, but to the contrary, in some cases has been found to be dramatically more coherent and clear.
        </p>
        <p>
          This allows technologies wishing to take advantage of the immense accelaration to integrate it into more downstream tasks like 3D object detection and zones requiring much faster results such as object detection by Vehicles and drones. While the accuracy most of the time remains at similar levels to current state-of-the-art frameworks, fields prioritising speed over cutting-edge accuracy may favor this system.
        </p>


        <h2> Important Concepts & Walkthrough of the process </h2>
        <!-- Visual Odometry and SLAM -->
        <h3>Visual Odometry and SLAM</h3>
        <p>
          Visual odometry(VO), one of the founding stones in computer vision, is the process of determining the position and orientation of an actor by analyzing the information provided by its sensors/cameras. Its uses vary immensely from the average drone to the Mars Rovers Exploration[<a href="#References">10</a>].
        </p>
        <p>
          There are many types of VO, we will be focusing on Monocular VO(opposed to Stereo VO) and visual-inertial odometry. Where Monocular VO refers to getting all the input from a single source (a single camera) and visual-inertial odometry to a system empolying an inertial measurement unit (IMU).
        </p>
        <p>
          Simultaneous localization and mapping (SLAM) as the name suggests involves a computational process of constructing a map of an unknown environment while simultaneously keeping track of the agent's location on said map.
        </p>

        <!-- 3D reconstruction -->
        <h3>3D reconstruction</h3>
        <p>
          Reconstruction refers to the process of capturing the shape and appearance of objects. This allows to determine the object's profile and creating a 3D grid.
          While many methods are possible to achieve this, the discussion primarily focuses on the Monocular aspect, also called Monocular cues methods. These refer to the usage of one or more images from one viewpoint to create a 3D construction. Exploiting 2D characteristics and defining features(such as shadows and textures) to define the object of a higher dimension.
        </p>

        <!-- TSDF -->
        <h3>TSDF Volume</h3>
        <p>
          A Truncated Signed Distance Field (TSDF) is a voxel array representing objects within a volume of space in which each voxel is labeled with the distance to the nearest surface. Numerous observations of an object from different perspectives average out noise and errors due to specular highlights and interreflections, producing a smooth continuous surface.<b> The TSDF algorithm can be efficiently parallelized on a general-purpose graphics processor, which allows data from RGB-D cameras to be integrated into the volume in real time.</b> The volume can be converted to a triangular mesh using the <b>Marching Cubes algorithm</b> (explained in "Depth maps and computations") and then handed off to application-specific processes.
        </p>
        <p>
          [<a href="#References">11</a>]A <i>d</i>-dimensional grid of voxels, where the position of each voxel <b>x</b> is defined by its center.
        </p>
        <img alt="tsdf1" class="functions" src="./images/tsdf1.png"/>
        <p class="explain">
          Function 1
        </p>
        <p>
          <i>sdf</i>, which is the signed distance in between voxel center and nearest object surface in direction of current measurement.
          Objects in front of itself are positive whereas the area inside the object is defined as negative. <i> i </i> refers to the <i> i </i>-th observation.
        </p>
        <p>
          <i> pic(<b>x</b>)</i> denotes to the projection of the voxel center onto the depth image. Accordingly <i> depth </i> is the measured depth in between the camera and the nearest object surface point. <i> cam </i> refers to the distance in between the voxel and the camera along the optical axis.
        </p>
        <p>
          The truncated variant of <i>sdf</i> is denoted by function 2:
        </p>
        <img alt="tsdf2" class="functions" src="./images/tsdf2.png"/>
        <p class="explain">
          Function 2
        </p>
        <p> Each voxel has a weight <i>w</i> atteched to it to asses uncertainty of the corresponding <i>sdf</i>.
        </p>
        <p>
          Finally, as mentioned prior, multiple viewpoints can be fused together in one <i>TSDF</i> to improve accuracy or added missing surface. This is done by weighted summation.
        </p>
        <img alt="tsdf3" class="functions" src="./images/tsdf3.png"/>
        <p class="explain">
          Function 3 and 4 respectively
        </p>
        <!-- Depth maps and computations -->
        <h3>Depth maps and computations</h3>
        <p>
          Depth maps are a vital instrument in attempting to recreate a model from images. They are clearly useful in a wide range of fields, however tend to set back the process' speed by their high number of computations. Most image-based real-time pipelines adopt the depth map fusion approach[<a href="#References">4</a>].
        </p>
        <p>
          Single-view depth maps are first estimated from each key frame and filtered for consistency and temporal smoothness, and lastely fused into a Truncated Signed Distance Function Volume (TSDF-V). <br>
          The reconstructed mesh can be then extracted from the fusion with the marching Cubes Algorithm [<a href="#References">5</a>]:

        </p>
        <!-- Marching Cubes Excourse -->
        <h5 style="margin-left:25px;"> Marching Cubes Algorithm </h5>
        <p class="exkurs">
           An Algorithm, originally intended for usage in medical imaging, it creates a tirangle mesh using an interative divide-and-conquer approach. It processes 3D data in-order and calculates verticies using linear interpolation. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data.
           <img alt="Cubes" class="results" src="./images/marchcubes.png"/>
           <p style="margin-left:3em;" class="explain">
             Figure 3. Triangulated Cubes-colorized[<a href="#References">5</a>]
           </p>
        </p>

        <p>
          This approach suffers as the maps are estimated individually on each key frame. Additionaly each map is estimated in full leading to substantial overlap in the results.
          Estimating the same surfaces multiple times causes unnecessary computations, and may cause difficulties delivering correct results in certain areas.(see Figure 1)
        </p>
        <p>
          The paper proposes a method to combat these problems fitting real-time reconstruction from a monocular video. By jointly reconstructing and fusing the 3D geometry directly in the TSDF-V and a given set of monocular images and camera poses from a SLAM system, NeuralRecon can incrementally unproject the image features to form sequentially a 3D feature volume by utilising convolutions to process the feature volume and eventually outputting a sparse TSDF-V which is then gradually refined.
        </p>
        <p>
          By directly reconstructing the implicit surface, the network is able to learn the local smoothness and global shape prior of natural 3D surfaces. This helps solve one of the issues introduced earlier of predicting separate depth maps.
        </p>

        <!-- GRU -->
        <h3>GRU</h3>
        <p>
          To make the jointly produced shape globally consistent with the previous reconstructions a learning based TSDF fusion module using the Gated Recurrent Unit(GRU) is proposed.
        </p>
        <p>
          While previous works commonly referred to SLTM for modeling, here a decision has been made to opt for a GRU based approach instead. GRU have been shown to be capabele of producing often comparable results with significantly faster computation times thanks to its simplicity. [<a href="#References">6,7</a>]
        </p>
        <img alt="GRU" class="results" src="./images/GRU.png">
        <p class="explain">
          Figure 2. LSTM and GRU graphical illustrations side by side [6].
        </p>
        <p>
          The GRU fusion makes the current-fragment reconstruction conditioned on the previously reconstructed global volume, hence creating a clear connection between the fragments correcting the problem, which appeared in other designs and yielding a joint reconstruction. This as a result, reconstructs a dense, accurate and globally coherent mesh. For exact calculation formulas see [6], however the main takeaway is, that similarly to the LSTM unit, the GRU has gating units that modulate the flow of information inside the unit, however, without having a separate memory cells. The GRU is forced to expose its full content and can control the flow of information when computing a new candidate.
        </p>
        <p>
          The experiments go on to report, that the new predicition procedure also removes the redundant computation in depth-based methods, which allows for the use of larger 3D CNNs while maintaining real-time performance.
        </p>

        <!-- CNN -->
        <h3>3D CNN</h3>
        <p>
          Convolutional neural network (CNN) is a form of an artificial neural network, specilized for their usefulness in analyzation of imagery in computer vision. A CNN has as the name suggests has convolutional layers. These recieve input from previous layers transform the input in some way and transfers it onwards. The Convolutional layers are able to detect patterns in the images fed to the network and learn the desired patterns. Becuase the layers pass one the results from one to another using a certain filter, bigger neural networks equal more defined results.
        </p>

        <!--  Convolution -->
        <h3>Convolution Operation</h3>
        <p>
          The action referred to as <i>convolving</i>, means the filter <i> convolves</i> across a certain block/volume of pixel/voxels from the input. The Convolution itself is a mathematical operation between two functions yielding a new third function, that is supposed to give a new representation of the relationship between the previous cubes.
        </p>
        <img alt="conv" class="functions" src="./images/Conv.svg"/>
        <p class="explain">
          Function 5
        </p>

        <!-- DATASETS -->
        <h3>Datasets</h3>
        <p>
          To validate the system, ScanNet and 7-Scenes datasets have been used. Using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks. However in this case it's put into action to validate the results from the experiments, measure the correctness and also see how the framework performs when placed in comparison to current state-of-the-art methods.
        </p>
        <img alt="ScanNet" class="results" src="./images/dataset1.jpg"/>
        <p class="explain">
          Figure 4. Example of ScanNet Dataset
        </p>
        <h5>ScanNet</h5>
        <p>
          ScanNet is a RGB-D video dataset containing 2.5M views in over a thousand scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations [<a href="#References">8</a>].
        </p>
        <h5>7-Scenes</h5>
        <p>
          The 7-Scenes dataset is a collection of tracked RGB-D camera frames. It can be used for evaluation of methods for uses such as dense tracking, mapping and more. [<a href="#References">9</a>]
        </p>


        <!-- Related Work -->
        <h2> Related Works </h2>
        <h3> Multi-view Depth Estimation </h3>
        <p>
          The proposed framework uses knowledge and baselines set up from multiple previous works.
        </p>
        <p>
          Before deep learning, renowned works in monocular 3D reconstruction have achieved good performance with <b>plane-sweeping</b> stereo and depth filters <b>under the assumption of photo-consistency</b>.
        </p>
        <p>
          Learning based methods on real time multi view depth estimation[<a href="#References">12,13</a>] attempt to <b>alleviate the photo consistency assumption</b> using 2D CNNs with a data driven approach, they maintain low cost and near real-time performance.
        </p>
        <p>
          For high-resolution images and when offline computation is possible, multi-view estimation is known as Multiple View Stereo (MVS). Learning based approaches[<a href="#References">14,15</a>] have been still limited to mid-resolution images due to memory constraints.
        </p>
        <p>
          SurfaceNet[<a href="#References">16</a>] has taken a new approach to the problem and used a unified volumetric representation to predict the volume occupancy. Atlas[<a href="#References">17</a>] has proposed using the volumetric design in predicting the TSDF and semantic labels with 3D CNN.
        </p>
        <p>
          A coarse-to-fine approach has been used in recent works to improve the pipeline created from the 3D CNNs.<br>
        </p>
        <h3> 3D Surface Reconstruction </h3>
        <p>
          When estimating the 3D surface position and producing the mesh an incremental TSDF-V fusion has been adopted in real time scenarios thanks to its simplicity and parallelization capability. Later on changed by RoutedFusion[<a href="#References">18</a>] from a simple linear addition into a data-dependent process.
        </p>
        <h3> Neural Implicit Representations </h3>
        <p>
          Recent works and the proposed system use a learned neural implicit representation by predicting the SDF with the NN from the encoded image features similar to PIFu[<a href="#References">19</a>] with the key difference that here sparse 3D convolutions are used to predict a discrete TSDF volume.
        </p>

        <!-- Algorithm -->
        <h2>Methods & Implementation</h2>
        <p>
          With the goal being a dense 3D scene reconstruction and have it be both accurate and in real time the system follows the concepts introduced earlier and the process seen in Figure 5.
          With a sequence of monocular images
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mo>{</mo>
            <msub>
              <mi>I</mi>
              <mn>t</mn>
            </msub>
            <mi>}</mi>
          </math>
          and a camera pose trajectory
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mo>{</mo>
            <msub>
              <mi>&xi;</mi>
              <mn>t</mn>
            </msub>
            <mi>}</mi>
          </math>
           provided by a SLAM system. NeuralRecon predicts a TSDF-V
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>S</mi>
               <mn>t</mn>
             </msub>
             <mn>g</mn>
           </msup>
           </math>
            with a three level coarse to fine approach that gradually increases the density of sparse voxels.
        </p>
        <img alt="Method" class="mainmethod" src="./images/method.png"/>
        <p class="explain">
          Figure 5.
        </p>
        <p>
          The 3D reconstruction is incremental and the input images are processed sequentially in local fragments of
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mo>N</mo>
          </math>
         Frames. From this incoming stream of images a few <b> Key Frame </b> images are selected as input for the NNs. After the selection process they are fed into the image backbone to have the multi-level features extracted. The TSDF-V of a local fragment
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msup>
            <msub>
              <mi>S</mi>
              <mn>t</mn>
            </msub>
            <mn>l</mn>
          </msup>
          </math>
          is simultaneously reconstructed and fused with the global volume
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msup>
            <msub>
              <mi>S</mi>
              <mn>t</mn>
            </msub>
            <mn>g</mn>
          </msup>
          </math>
          by a learning-based approach.
        </p>
        <p>
          The reconstruction occurs fragment for fragment, with a window of
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mo>N</mo>
          </math>
           key frames being a local fragment. These are then back-projected along each ray into the 3D feature volume
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>F</mi>
               <mn>t</mn>
             </msub>
             <mn>l</mn>
           </msup>
           </math>
           The feature volume is obtained by averaging the features from different views according to the visibility weight of each voxel. Which is the number of views from which a voxel can be observed in that local fragment. This is seen in Figure 6.
        </p>
        <img alt="Method2" class="mainmethod" src="./images/method2.png"/>
        <p class="explain">
          Figure 6.
        </p>
        <p>
          The MLP used predicts both the occupancy score
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mn>o</mn>
          </math>
          and the SDF value
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mn>x</mn>
          </math> of each voxel in each local TSDF-V. The sparsity aspect of the TSDF-V comes from this occupancy score which is checked for a lower bound
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mn>&theta;</mn>
          </math>
          and turned to void space if its less likely to be within the TSDF truncation distance. Afterwards the local volume is upsampled and concatenated with the next local feature volume and given as input to the GRU fusion.
        </p>
        <p>
          The GRU fusion maintains the consistency between fragments during reconstruction.
          The reconstruction is continuous, taking into equation the results of previous reconstructions. The GRU used is one with a 3D convolutional module. The sparse convolutions extract the 3D geometric features
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msup>
            <msub>
              <mi>G</mi>
              <mn>t</mn>
            </msub>
            <mn>l</mn>
          </msup>
          </math>
           and the hidden state
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>H</mi>
               <mn>t-1</mn>
             </msub>
             <mn>g</mn>
           </msup>
           </math>
        </p>




        <!-- Experiments -->
        <h2>Experiments & Results</h2>
        ...

        <!-- Experiments -->
        <h2>Conclusion</h2>
        ...

        <!-- References -->
        <div id="References">
        <h5><a href="#">References</a></h5>
        <ol>
            <li><a href="https://arxiv.org/pdf/2104.00681.pdf">Jiaming Sun, Yiming Xie1,Linghao Chen, Xiaowei Zhou, Hujun Bao, Zhejiang University, SenseTime Research - NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video</a></li>
            <li><a href="https://zju3dv.github.io/neuralrecon/">NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video Presentation Page</a></li>
            <li><a href="https://arxiv.org/pdf/2007.11898.pdf">Carlos Campos, Richard Elvira, Juan J. G´omez Rodr´ıguez, Jos´e M. M. Montiel, and Juan D. Tard´os. ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. ArXiv, 2020.</a></li>
            <li><a href="https://www.researchgate.net/publication/284139276_3D_Modeling_on_the_Go_Interactive_3D_Reconstruction_of_Large-Scale_Scenes_on_Mobile_Devices">Thomas Schops, Torsten Sattler, Christian Hane, and Marc
              Pollefeys. 3D Modeling on the Go: Interactive 3D Reconstruction
              of Large-Scale Scenes on Mobile Devices. In 3DV,
              2015.</a></li>
            <li><a href="https://dl.acm.org/doi/pdf/10.1145/37402.37422">William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. SIGGRAPH, 1987</a></li>
            <li><a href="https://arxiv.org/pdf/1412.3555.pdf">Junyoung Chung Caglar Gulcehre, Kyung Hyun Cho Universite de Montreal, Yoshua Bengio Universite de Montreal CIFAR Senior Fellow Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
            </a></li>
            <li><a href="https://arxiv.org/pdf/1409.1259.pdf">Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
            </a></li>
            <li><a href="https://arxiv.org/pdf/1702.04405">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner
            </a></li>
            <li>Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images. In CVPR, 2013</li>
            <li><a href="https://onlinelibrary.wiley.com/doi/10.1002/rob.20184">Maimone, M.; Cheng, Y.; Matthies, L. (2007). "Two years of Visual Odometry on the Mars Exploration Rovers"</a></li>
            <li><a href="https://www.researchgate.net/publication/268513443_Truncated_Signed_Distance_Function_Experiments_on_Voxel_Size">Werner, Diana & Al-Hamadi, Ayoub & Werner, Philipp. (2014). Truncated Signed Distance Function: Experiments on Voxel Size. 8815. 357-364. 10.1007/978-3-319-11755-3_40.</a></li>
            <li>Kaixuan Wang and Shaojie Shen. MVDepthNet: Real-Time Multiview Depth Estimation Neural Network. In 3DV, 2018.</li>
            <li>Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa G Narasimhan, and Jan Kautz. Neural RGB-> D Sensing: Depth and uncertainty from a video camera. In CVPR, 2019. </li>
            <li>Henrik Aanæs, Rasmus Ramsbøl Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-Scale Data for Multiple-View Stereopsis. IJCV, 2016.</li>
            <li>Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction. ACM TOG, 2017. </li>
            <li>Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. SurfaceNet: An end-to-end 3D neural network for multiview stereopsis. In ICCV, 2017.</li>
            <li>Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha,
              Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: Endto-
              End 3D Scene Reconstruction from Posed Images. In
              ECCV, 2020. </li>
            <li>Silvan Weder, Johannes Schönberger, Marc Pollefeys, and Martin R. Oswald. RoutedFusion: Learning Real-Time Depth Map Fusion. In CVPR, 2020.</li>
            <li>Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization.In ICCV, 2019.</li>
          </ol>
        </div>
      </section>


    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
        <footer class="inner">
          <p class="copyright">Theme heavily inspired by <a href="https://github.com/jasoncostello">Jason Costello</a></p>
          <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
        </footer>
      </div>

  </body>
</html>
