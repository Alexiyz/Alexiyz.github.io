<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Alexiy's Blog Page" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <link href='https://fonts.googleapis.com/css?family=Roboto:400,400italic,700italic,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Condensed:300,300italic,700,700italic' rel='stylesheet' type='text/css'>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <title>Paper Analysis: NeuralRecon</title>
  </head>

  <body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
        </header>
    </div>

    <!-- CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <!-- TITLE -->
        <img alt="a Wild Me appears!" class="myimage" src="./images/me.jpeg"/>
        <p id="name">Alexiy Zhandarov</p>
        <p id="undername">Informatics B.Sc Student</p>

        <a id="home_link" href="index.html"><u>Home</u></a>
        <a id="home_link" href="#References"><u>Bottom</u></a>
        <hr id=startmargin>

        <!-- ACTUAL START -->
        <h1 style="text-align:center">Paper Analysis on Real-Time 3D Scene Reconstruction from Monocular Video</h1>
        <p class="time"> May 20, 2022 </p>

        <!-- QUICK INTRO -->
        <p> In this blog post, I will review and analyze the paper NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video published in CVPR 2021 [<a href="#References">1</a>]
        After a brief introduction and going over the relevant terms, I will attempt to explain the algorithm and framework as simply as possible, observe an example, and finally dissect the results and see what it all means for the future of computer vision. </p>

        <video width="100%" height="250" autoplay muted>
          <source src="./images/web-scene2.m4v" type="video/mp4">
              Your browser does not support the video format.
              Try Using Chrome on PC.
          </video>
          <p class="explain"> Showcase of the proposed framework.[<a href="#References">2</a>] </p>



        <h3 class="contents_header"> Contents</h5>
        <p class="contents">
        I. Introduction<br>
        II.<a href="#contrib"> Main Contributions</a><br>
        III. <a href="#conc">Important concepts and walkthrough of the procedure</a><br>
        IV. <a href="#datasets">Datasets </a><br>
        V. <a href="#works">Related Works</a><br>
        VI. <a href="#method">Method & Implementation </a><br>
        VII. <a href="#experi">Experiments & Results</a><br>
        VIII. <a href="#conclus">Conclusion & personal remarks</a><br>
        IX. <a href="#References">References</a> <br>
        </p>



        <!-- INTRODUCTION -->
        <h2 id="intro">Introduction</h2>
        <p>
          Scene reconstruction is one of the central tasks in 3D computer vision with currently numerous applications. 3D reconstruction has seen a wide range of usage within the medical industry, computer animation and graphics, virual reality(VR) and augmented reality(AR).
        </p>
        <p>
          Focusing in on the latter for example, to enable realistic and immersive interactions between the desired effects and the surrounding physical environment, the 3D reconstruction has many standards to fulfil; it needs to be accurate, coherent and performed without any noticeable delay.
        </p>
        <p>Many of these reconstructions are often estimated by fusing depth measurements from special sensors, such as structured light, time of flight, or others into 3D models. While these can be extremely effective, they require special hardware making them more cumbersome and expensive than systems that rely solely on RGB cameras.</p>
        <p>
          Another aspect to consider is the real-time aspect.
          While camera motion can be tracked accurately with state-of-the-art visual-inertial SLAM systems [<a href="#References">3</a>], real-time image-based dense reconstruction remains to be a challenging problem due to <b>low reconstruction quality and high computation demands.</b> Slow and low quality algorithms are naturaly undesirable and can be unreliable when requiring precise results. The proposed framework 'NeuralRecon' attempts to address these issues. Moreover the method proposes a way avoiding the common path of depth-based methods, which uses TSDF fusion and per frame analyzation, and rather predicts it in a local window.
        </p>
        <img alt="result1" class="results" src="./images/results1.jpg"/>
        <p class="explain">
          Figure 1. Comparison between depth-based methods and the proposed 3D reconstruction method.
        </p>



        <!-- Main Contributions -->
        <h2 id="contrib">Main Contributions</h2>
        <p>
          NeuralRecon is a relatively new work being introduced at CVPR 2021, but is already showing some impressive results reconstructing 3D indoor scenes at real time and with high quality, being compratively equal or better than the current state-of-the-art frameworks. Here the aspect of speed comes as most impressive. Through a new prediction based procedure replacing the old depth maps used in similar works the time needed to generate coherent 3D models has been drastically reduced. (See Figure 1), Most importantly, with this new found speed, and quality of the constructions has not suffered, but to the contrary, in some cases has been found to be dramatically more coherent and clear.
        </p>
        <p>
          This allows technologies wishing to take advantage of the immense accelaration to integrate it into more downstream tasks like 3D object detection and zones requiring much faster results such as object detection by Vehicles and drones. While the accuracy most of the time remains at similar levels to current state-of-the-art frameworks, fields prioritising speed over cutting-edge accuracy may favor this system. Moreover it has been found that the proposed framework is able to execute with much greater graphical memory efficiency, playing favourably into integration of this algorithm into less hardware-equiped systems.
        </p>




        <h2 id="conc"> Important Concepts & Walkthrough of the process </h2>
        <!-- Visual Odometry and SLAM -->
        <h3>Visual Odometry and SLAM</h3>
        <p>
          Visual odometry(VO), one of the founding stones in computer vision, is the process of determining the position and orientation of an actor by analyzing the information provided by its sensors/cameras. Its uses vary immensely from the average drone to the Mars Rovers Exploration[<a href="#References">10</a>].
        </p>
        <p>
          There are many types of VO, we will be focusing on Monocular VO(opposed to Stereo VO) and visual-inertial odometry. Monocular VO refers to getting all the input from a single source (a single camera) and visual-inertial odometry to a system empolying an inertial measurement unit (IMU).
        </p>
        <p>
          Simultaneous localization and mapping (SLAM) as the name suggests involves a computational process of constructing a map of an unknown environment while simultaneously keeping track of the agent's location on said map.
        </p>

        <!-- 3D reconstruction -->
        <h3>3D reconstruction</h3>
        <p>
          Reconstruction refers to the process of capturing the shape and appearance of objects. This allows to determine the object's profile and creating a 3D grid.
          While many methods are possible to achieve this, the discussion primarily focuses on the Monocular aspect, also called Monocular cues methods. These refer to the usage of one or more images from one viewpoint to create a 3D construction. Exploiting 2D characteristics and defining features(such as shadows and textures) to define the object of a higher dimension.
        </p>

        <!-- TSDF -->
        <h3>TSDF Volume</h3>
        <p>
          A Truncated Signed Distance Field (TSDF) is a voxel array representing objects within a volume of space in which each voxel is labeled with the distance to the nearest surface. Numerous observations of an object from different perspectives average out noise and errors due to specular highlights and interreflections, producing a smooth continuous surface.<b> The TSDF algorithm can be efficiently parallelized on a general-purpose graphics processor, which allows data from RGB-D cameras to be integrated into the volume in real time.</b> The volume can be converted to a triangular mesh using the <b>Marching Cubes algorithm</b> (explained in "Depth maps and computations") and then handed off to application-specific processes.
        </p>
        <p>
          [<a href="#References">11</a>]A <i>d</i>-dimensional grid of voxels, where the position of each voxel <b>x</b> is defined by its center.
        </p>
        <img alt="tsdf1" class="functions" src="./images/tsdf1.png"/>
        <p class="explain">
          Function 1
        </p>
        <p>
          <i>sdf</i>, which is the signed distance in between voxel center and nearest object surface in direction of current measurement.
          Objects in front of itself are positive whereas the area inside the object is defined as negative. <i> i </i> refers to the <i> i </i>-th observation.
        </p>
        <p>
          <i> pic(<b>x</b>)</i> denotes to the projection of the voxel center onto the depth image. Accordingly <i> depth </i> is the measured depth in between the camera and the nearest object surface point. <i> cam </i> refers to the distance in between the voxel and the camera along the optical axis.
        </p>
        <p>
          The truncated variant of <i>sdf</i> is denoted by function 2:
        </p>
        <img alt="tsdf2" class="functions" src="./images/tsdf2.png"/>
        <p class="explain">
          Function 2
        </p>
        <p> Each voxel has a weight <i>w</i> atteched to it to asses uncertainty of the corresponding <i>sdf</i>.
        </p>
        <p>
          Finally, as mentioned prior, multiple viewpoints can be fused together in one <i>TSDF</i> to improve accuracy or added missing surface. This is done by weighted summation.
        </p>
        <img alt="tsdf3" class="functions" src="./images/tsdf3.png"/>
        <p class="explain">
          Function 3 and 4 respectively
        </p>
        <!-- Depth maps and computations -->
        <h3>Depth maps and computations</h3>
        <p>
          Depth maps are a vital instrument in attempting to recreate a model from images. They are clearly useful in a wide range of fields, however tend to set back the process' speed by their high number of computations. Most image-based real-time pipelines adopt the depth map fusion approach[<a href="#References">4</a>].
        </p>
        <p>
          Single-view depth maps are first estimated from each key frame and filtered for consistency and temporal smoothness, and lastely fused into a Truncated Signed Distance Function Volume (TSDF-V). <br>
          The reconstructed mesh can be then extracted from the fusion with the marching Cubes Algorithm [<a href="#References">5</a>]:

        </p>
        <!-- Marching Cubes Excourse -->
        <h5 style="margin-left:25px;"> Marching Cubes Algorithm </h5>
        <p class="exkurs">
           An Algorithm, originally intended for usage in medical imaging, it creates a tirangle mesh using an interative divide-and-conquer approach. It processes 3D data in-order and calculates verticies using linear interpolation. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data.
           <img alt="Cubes" class="results" src="./images/marchcubes.png"/>
           <p style="margin-left:3em;" class="explain">
             Figure 3. Triangulated Cubes-colorized[<a href="#References">5</a>]
           </p>
        </p>

        <p>
          This approach suffers as the maps are estimated individually on each key frame. Additionaly each map is estimated in full leading to substantial overlap in the results.
          Estimating the same surfaces multiple times causes unnecessary computations, and may cause difficulties delivering correct results in certain areas.(see Figure 1)
        </p>
        <p>
          The paper proposes a method to combat these problems fitting real-time reconstruction from a monocular video. By jointly reconstructing and fusing the 3D geometry directly in the TSDF-V and a given set of monocular images and camera poses from a SLAM system, NeuralRecon can incrementally unproject the image features to form sequentially a 3D feature volume by utilising convolutions to process the feature volume and eventually outputting a sparse TSDF-V which is then gradually refined.
        </p>
        <p>
          By directly reconstructing the implicit surface, the network is able to learn the local smoothness and global shape prior of natural 3D surfaces. This helps solve one of the issues introduced earlier of predicting separate depth maps.
        </p>

        <!-- GRU -->
        <h3>GRU</h3>
        <p>
          To make the jointly produced shape globally consistent with the previous reconstructions a learning based TSDF fusion module using the Gated Recurrent Unit(GRU) is proposed.
        </p>
        <p>
          While previous works commonly referred to SLTM for modeling, here a decision has been made to opt for a GRU based approach instead. GRU have been shown to be capabele of producing often comparable results with significantly faster computation times thanks to its simplicity. [<a href="#References">6,7</a>]
        </p>
        <img alt="GRU" class="results" src="./images/GRU.png">
        <p class="explain">
          Figure 2. LSTM and GRU graphical illustrations side by side [6].
        </p>
        <p>
          The GRU fusion makes the current-fragment reconstruction conditioned on the previously reconstructed global volume, hence creating a clear connection between the fragments correcting the problem, which appeared in other designs and yielding a joint reconstruction. This as a result, reconstructs a dense, accurate and globally coherent mesh. For exact calculation formulas see [6], however the main takeaway is, that similarly to the LSTM unit, the GRU has gating units that modulate the flow of information inside the unit, however, without having a separate memory cells. The GRU is forced to expose its full content and can control the flow of information when computing a new candidate.
        </p>
        <p>
          The experiments go on to report, that the new predicition procedure also removes the redundant computation in depth-based methods, which allows for the use of larger 3D CNNs while maintaining real-time performance.
        </p>

        <!-- CNN -->
        <h3>3D CNN</h3>
        <p>
          Convolutional neural network (CNN) is a form of an artificial neural network, specilized for their usefulness in analyzation of imagery in computer vision. A CNN has as the name suggests has convolutional layers. These recieve input from previous layers transform the input in some way and transfers it onwards. The Convolutional layers are able to detect patterns in the images fed to the network and learn the desired patterns. Becuase the layers pass one the results from one to another using a certain filter, bigger neural networks equal more defined results.
        </p>

        <!--  Convolution -->
        <h3>Convolution Operation</h3>
        <p>
          The action referred to as <i>convolving</i>, means the filter <i> convolves</i> across a certain block/volume of pixel/voxels from the input. The Convolution itself is a mathematical operation between two functions yielding a new third function, that is supposed to give a new representation of the relationship between the previous cubes.
        </p>
        <img alt="conv" class="functions" src="./images/Conv.svg"/>
        <p class="explain">
          Function 5
        </p>



        <!-- DATASETS -->
        <h2 id="datasets">Datasets</h2>
        <p>
          To validate the system, ScanNet and 7-Scenes datasets have been used. Using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks. However in this case it's put into action to validate the results from the experiments, measure the correctness and also see how the framework performs when placed in comparison to current state-of-the-art methods.
        </p>
        <img alt="ScanNet" class="results" src="./images/dataset1.jpg"/>
        <p class="explain">
          Figure 4. Example of ScanNet Dataset
        </p>
        <h5>ScanNet</h5>
        <p>
          ScanNet is a RGB-D video dataset containing 2.5M views in over a thousand scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations [<a href="#References">8</a>].
        </p>
        <h5>7-Scenes</h5>
        <p>
          The 7-Scenes dataset is a collection of tracked RGB-D camera frames. It can be used for evaluation of methods for uses such as dense tracking, mapping and more. [<a href="#References">9</a>]
        </p>



        <!-- Related Work -->
        <h2 id="works"> Related Works </h2>
        <h3> Multi-view Depth Estimation </h3>
        <p>
          The proposed framework uses knowledge and baselines set up from multiple previous works.
        </p>
        <p>
          Before deep learning, renowned works in monocular 3D reconstruction have achieved good performance with <b>plane-sweeping</b> stereo and depth filters <b>under the assumption of photo-consistency</b>.
        </p>
        <p>
          Learning based methods on real time multi view depth estimation[<a href="#References">12,13</a>] attempt to <b>alleviate the photo consistency assumption</b> using 2D CNNs with a data driven approach, they maintain low cost and near real-time performance.
        </p>
        <p>
          For high-resolution images and when offline computation is possible, multi-view estimation is known as Multiple View Stereo (MVS). Learning based approaches[<a href="#References">14,15</a>] have been still limited to mid-resolution images due to memory constraints.
        </p>
        <p>
          SurfaceNet[<a href="#References">16</a>] has taken a new approach to the problem and used a unified volumetric representation to predict the volume occupancy. Atlas[<a href="#References">17</a>] has proposed using the volumetric design in predicting the TSDF and semantic labels with 3D CNN.
        </p>
        <p>
          A coarse-to-fine approach has been used in recent works to improve the pipeline created from the 3D CNNs.<br>
        </p>
        <h3> 3D Surface Reconstruction </h3>
        <p>
          When estimating the 3D surface position and producing the mesh an incremental TSDF-V fusion has been adopted in real time scenarios thanks to its simplicity and parallelization capability. Later on changed by RoutedFusion[<a href="#References">18</a>] from a simple linear addition into a data-dependent process.
        </p>
        <h3> Neural Implicit Representations </h3>
        <p>
          Recent works and the proposed system use a learned neural implicit representation by predicting the SDF with the NN from the encoded image features similar to PIFu[<a href="#References">19</a>] with the key difference that here sparse 3D convolutions are used to predict a discrete TSDF volume.
        </p>



        <!-- Algorithm -->
        <h2 id="method">Methods & Implementation</h2>
        <p>
          With the goal being a dense 3D scene reconstruction and have it be both accurate and in real time the system follows the concepts introduced earlier and the process seen in Figure 5 to achieve this.

          With a sequence of monocular images
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mo>{</mo>
            <msub>
              <mi>I</mi>
              <mn>t</mn>
            </msub>
            <mi>}</mi>
          </math>

          and a camera pose trajectory
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mo>{</mo>
            <msub>
              <mi>&xi;</mi>
              <mn>t</mn>
            </msub>
            <mi>}</mi>
          </math>

           provided by a SLAM system. NeuralRecon predicts a TSDF-V
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>S</mi>
               <mn>t</mn>
             </msub>
             <mn>g</mn>
           </msup>
           </math>

            with a three level coarse to fine approach that gradually increases the density of sparse voxels.
        </p>
        <img alt="Method" class="mainmethod" src="./images/method.png"/>
        <p class="explain">
          Figure 5. (Notice: g stands for global, l for current level, t for time)
        </p>
        <p>
          The 3D reconstruction is incremental and the input images are processed sequentially in local fragments of
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mo>N</mo>
          </math>

         Frames. From this incoming stream of images a few <b> Key Frames </b> are selected as input for the NNs. The frames are then defined as an enclosed Fragment Bounding Volume (FBV) and it encloses all their view frustums aswell. Only the area within this FBV is considered during a fragment-reconstruction.
       </p>

       <p>After the selection process they are fed into the image backbone to have the multi-level features extracted. The TSDF-V of a local fragment
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msup>
            <msub>
              <mi>S</mi>
              <mn>t</mn>
            </msub>
            <mn>l</mn>
          </msup>
          </math>
          is simultaneously reconstructed fragment for fragment and fused with the global volume
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msup>
            <msub>
              <mi>S</mi>
              <mn>t</mn>
            </msub>
            <mn>g</mn>
          </msup>
          </math>
          by a learning-based approach.
        </p>
        <p>
          The features extracted are then back-projected along each ray into the 3D feature volume
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>F</mi>
               <mn>t</mn>
             </msub>
             <mn>l</mn>
           </msup>
         </math>.
           The feature volume is obtained by averaging the features from different views according to the visibility weight of each voxel. Which is the number of views from which a voxel can be observed in that local fragment (This is seen in Figure 6).
        </p>
        <img alt="Method2" class="mainmethod" src="./images/method2.png"/>
        <p class="explain">
          Figure 6.
        </p>
        <p>
          The resulting
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msup>
            <msub>
              <mi>F</mi>
              <mn>t</mn>
            </msub>
            <mn>l</mn>
          </msup>
        </math>s are then fed into the GRU Fusion and later to the MLP.
      </p>

      <!-- The MLP Part-->
      <p>
          The MLP used predicts both the occupancy score
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mn>o</mn>
          </math>
          and the SDF value
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mn>x</mn>
          </math>
          of each voxel in each local TSDF-V. The sparsity aspect of the TSDF-V comes from this occupancy score, which is checked for a lower bound
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mn>&theta;</mn>
          </math>
          and turned to void space if its less likely to be within the TSDF truncation distance. Afterwards the local volume is upsampled and concatenated with the next local feature volume, and given as input to the GRU fusion. This process repeats afterwards again.
        </p>

        <!-- The GRU Part-->
        <p>
          The GRU fusion maintains the consistency between fragments during reconstruction.
          The reconstruction is continuous, taking into equation the results of previous reconstructions. The GRU used is one with a 3D convolutional module, where sparse convolutions extract the 3D geometric features
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msup>
            <msub>
              <mi>G</mi>
              <mn>t</mn>
            </msub>
            <mn>l</mn>
          </msup>
        </math>
        . The geometric features are fused with the extracted local hidden state
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>H</mi>
               <mn>t-1</mn>
             </msub>
             <mn>l</mn>
           </msup>
           </math>
           and the GRU produces the new
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>H</mi>
               <mn>t</mn>
             </msub>
             <mn>l</mn>
           </msup>
           </math>
           , to be passed through the MLP and predict the TSDF-V of the corresponding layer.
           The hidden state will also replace the global hidden state
           <math xmlns="http://www.w3.org/1998/Math/MathML">
             <msup>
             <msub>
               <mi>H</mi>
               <mn>t</mn>
             </msub>
             <mn>g</mn>
           </msup>
           </math>
           for the next repetition.
           <img alt="gru_state_fusion" class="functions" src="./images/gru_state_fusion.png"/>
          <!-- Functions explanation -->
           <p class="explain">
             Functions 6,7,8 and 9 respectively.<br>
             Formally, denoting
             <math xmlns="http://www.w3.org/1998/Math/MathML">
               <msub>
                 <mi>z</mi>
                 <mn>t</mn>
               </msub>
             </math>
              as the update gate,
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <msub>
                  <mi>r</mi>
                  <mn>t</mn>
                </msub>
              </math>
               as the reset gate,
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <mi>&sigma;</mi>
               </math>
                as the sigmoid function and
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                 <msub>
                   <mi>W</mi>
                   <mn>*</mn>
                 </msub>
               </math>
               as the weight for sparse convolution.
           </p>

           <p>
             At our last steps in this coarse-to-fine design, the last TSDF-V is integrated into the global one and the Marching Cubes algorithm is performed on this global volume to create the mesh.

        <!-- Experiments -->
        <h2 id="experi">Experiments & Results</h2>
        <p>
        In order to evaluate the this new design and the quality it manages to output a few experiments have been conducted.
        The experiments were on two indoor datasets, <a href="#datasets"> ScanNet(V2) and 7-Scenes </a>. The researchers used the same training and validation data used in previous works [<a href="#References">17</a>, <a href="#References">21</a>] with a corresponding baseline to make a fair comparison. The framework is first trained with the ScanNet dataset and then validated on 7-Scenes.
        </p>
        <p>
          The 3D reconstruction quality is evaluated using 3D geometry metrics presented in [<a href="#References">17</a>]
          and the 2D depth metrics defined in [<a href="#References">21</a>]
        </p>
        <img alt="metrics" class="results" src="./images/metrics.png">
        <p class="explain">
          Table 1. Metric definitions.
          <i>n</i> is the number of pixels for the ground truth and prediction<br>
          <i>d</i> is the predicted and ground truth depth.<br>
          <i>t</i> is the predicted and ground truth TSDFs.<br>
          <i>p</i> is the predicted and ground truth point clouds
        </p>
        <p>
          The main copmarisons being made are between the discussed method and the following baseline methods in these categories:<br>
          1) Real-time methods for multi-view depth estimation.<br>
          2) Multiple view of NeuralRecon given the same efficiency.<br>
          3) Learning based SLAM methods.<br>
        </p>
        <p>
          The copmarisons made are goaled at demonstrating the reconstruction accuracy of NeuralRecon. Additionaly that the framework does not suffer from its real-time based aspect and a compare the system to other methods performing reconstruction and estimations simultaneously.
        </p>
        <img alt="results2" class="results" src="./images/results2.png">
        <p class="explain">
          Table 2. 3D geometry metrics on ScanNet.
        </p>
        <img alt="results3" class="results" src="./images/results3.png">
        <p class="explain">
          Table 3. 2D geometry metrics on ScanNet.
        </p>
        <p>
          Based on Table 2, we can see that the framework produces much better performance than recent learning-based methods and achieves slightly better results than COLMAP. While the tables speak strongly for themselves it's nevertheless important to mention that the method manages to surpass the volumetric baseline method Atlas on the accuracy, precision, and F-score scales. Where the shine falls is on terms of completeness and recallability, showing inferior performance compared to both depth-based methods and Atlas. Being an offline approach, Atlast has the advantagge of having a global context prior to its predictions. This leads to sometimes better results than the ground-truth on their part, due to its TSDF completion capability, this is though, still yet to be perfected showing also oversmoothend gemometries and inaccuracies.
        </p>
        <p>
          In terms of the 2D metrics, NeuralRecon also outperforms previous state-of-the-art methods for almost all 2D depth matrics.
        </p>
        <img alt="results4" class="results" src="./images/results4.png">
        <p class="explain">
          Table 4. 3D geometry metrics (top block), 2D depth metrics (bottom block) on 7-Scenes. Time in ms.
        </p>
        <p>
          As for the evalution on the 7-Scenes dataset, the method achieves comparable performance to CNMNet (state-of-the-art learning-depth-estimation method) and outperforms the rest.
        </p>
        <p>
          Being a real-time method, perhaps would be most significant to mention is its efficiency. NeuralRecon works at a time cost of 30ms per key frame, achieving real-time speed at 33 key frames per second and outperforming all previous methods. This is about 10 times faster than Atlas. This is very much thanks to the prediction process removing unnecessary computations present in other methods.
        </p>
        <img alt="results5" class="imagecomp" src="./images/results5.png">
        <p class="explain">
          Figure 7. Qualitative results on ScanNet. The color indicates surface normal. (See the original paper for the high resolution comparison)
        </p>
        <p>
          As far as quality comes, compared to depth-based methods, NeuralRecon can produce much more coherent reconstruction results. The method also recovers sharper geometry compared to Atlas, which illustrates the effectiveness of the local fragment design in our method. Reconstructing only within the local fragment window avoids irrelevant image features from far-away camera views to be fused into the 3D volume.
        </p>
        <!-- Conclusion -->
        <h2 id="conclus">Conclusion & personal remarks</h2>
        <p>
          NeuralRecon proposes a new novel system for real-time 3D reconstruction with monocular video.
          It uses joint reconstruction and sparse TSDF fusion to efficiently deliver a 3D mesh in real time, faster than any state-of-the-art technology before it. 3D sparse convolutions and GRU play a key role, as seen in the ablation study and can be viewed as the improving components to similar algorithms such as Atlas, coming before it.
          While improving the speed, it's  accuracy has remained at top level.
          Old pipelines have been removed, and in their stead, the framework takes advantage of the power of neural networks and the learning element able to predict depth maps.
          Despite all this, further improvement remain possible; such as improving the accuracy further with methods seen in CNMNet.
          This has been a big step into fitting the framework and enabling real-time 3D recosntructions for commercial use and for downstream tasks and new possibilities that were locked priorly behind efficiency-gates or the graphical power needed.
        </p>
        <p>
          It remains important to mention that the method and its testing have been limited to indoor scenes.<br>
          While fitting for some, many other fields require outdoors usage or at the very least outdoors compatibility. The Testing aswell has been done on a single system; Different GPUs optimized for different tasks may yield different results. The results as they are, are quite ground-breaking, and can pave the way for further optimizations and reconstructions algorithms to really highlight the usefulness. While the creators address the topic of outdoor scenes in an outside source, this remains to be untested due to ScanNet dataset being exclusively indoors. A big point I found personaly interesting and left almost unmentioned is the GRAM used has been reduced by around 8x. This immense drop opens the door for personal testing/experimenting. (from over 24GB before for a scene to around 3GB), a huge step towards enabling commercial usage.
        </p>

        <!-- References -->
        <div id="References">
        <h4><a href="#">References</a></h4>
        <ol>
            <li><a href="https://arxiv.org/pdf/2104.00681.pdf">Jiaming Sun, Yiming Xie1,Linghao Chen, Xiaowei Zhou, Hujun Bao, Zhejiang University, SenseTime Research - NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video</a></li>
            <li><a href="https://zju3dv.github.io/neuralrecon/">NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video Presentation Page</a></li>
            <li><a href="https://arxiv.org/pdf/2007.11898.pdf">Carlos Campos, Richard Elvira, Juan J. G´omez Rodr´ıguez, Jos´e M. M. Montiel, and Juan D. Tard´os. ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM. ArXiv, 2020.</a></li>
            <li><a href="https://www.researchgate.net/publication/284139276_3D_Modeling_on_the_Go_Interactive_3D_Reconstruction_of_Large-Scale_Scenes_on_Mobile_Devices">Thomas Schops, Torsten Sattler, Christian Hane, and Marc
              Pollefeys. 3D Modeling on the Go: Interactive 3D Reconstruction
              of Large-Scale Scenes on Mobile Devices. In 3DV,
              2015.</a></li>
            <li><a href="https://dl.acm.org/doi/pdf/10.1145/37402.37422">William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. SIGGRAPH, 1987</a></li>
            <li><a href="https://arxiv.org/pdf/1412.3555.pdf">Junyoung Chung Caglar Gulcehre, Kyung Hyun Cho Universite de Montreal, Yoshua Bengio Universite de Montreal CIFAR Senior Fellow Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
            </a></li>
            <li><a href="https://arxiv.org/pdf/1409.1259.pdf">Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
            </a></li>
            <li><a href="https://arxiv.org/pdf/1702.04405">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner
            </a></li>
            <li>Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images. In CVPR, 2013</li>
            <li><a href="https://onlinelibrary.wiley.com/doi/10.1002/rob.20184">Maimone, M.; Cheng, Y.; Matthies, L. (2007). "Two years of Visual Odometry on the Mars Exploration Rovers"</a></li>
            <li><a href="https://www.researchgate.net/publication/268513443_Truncated_Signed_Distance_Function_Experiments_on_Voxel_Size">Werner, Diana & Al-Hamadi, Ayoub & Werner, Philipp. (2014). Truncated Signed Distance Function: Experiments on Voxel Size. 8815. 357-364. 10.1007/978-3-319-11755-3_40.</a></li>
            <li>Kaixuan Wang and Shaojie Shen. MVDepthNet: Real-Time Multiview Depth Estimation Neural Network. In 3DV, 2018.</li>
            <li>Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa G Narasimhan, and Jan Kautz. Neural RGB-> D Sensing: Depth and uncertainty from a video camera. In CVPR, 2019. </li>
            <li>Henrik Aanæs, Rasmus Ramsbøl Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-Scale Data for Multiple-View Stereopsis. IJCV, 2016.</li>
            <li>Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction. ACM TOG, 2017. </li>
            <li>Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. SurfaceNet: An end-to-end 3D neural network for multiview stereopsis. In ICCV, 2017.</li>
            <li><a href="https://arxiv.org/pdf/2003.10432.pdf">Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: Endto-End 3D Scene Reconstruction from Posed Images. In ECCV, 2020</a></li>
            <li>Silvan Weder, Johannes Schönberger, Marc Pollefeys, and Martin R. Oswald. RoutedFusion: Learning Real-Time Depth Map Fusion. In CVPR, 2020.</li>
            <li><a href="https://arxiv.org/pdf/1905.05172.pdf">Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization.In ICCV, 2019</a></li>
            <li><a href="https://arxiv.org/pdf/1806.04807.pdf">Chengzhou Tang and Ping Tan. BA-Net: Dense Bundle Adjustment Networks. In ICLR, 2019 </a></li>
            <li><a href="https://papers.nips.cc/paper/2014/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf">David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NeurIPS, 2014</a></li>
          </ol>
        </div>
      </section>


    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
        <footer class="inner">
          <p class="copyright">Theme heavily inspired by <a href="https://github.com/jasoncostello">Jason Costello</a></p>
          <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
        </footer>
      </div>

  </body>
</html>
